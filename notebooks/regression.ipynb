{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GP Regression with GPflow\n",
    "--\n",
    "\n",
    "*James Hensman, 2015, 2016*\n",
    "\n",
    "GP regression (with Gaussian noise) is the most straightforward GP model in GPflow. Because of the conjugacy of the latent process and the noise, the marginal likelihood $p(\\mathbf y\\,|\\,\\theta)$ can be computed exactly.\n",
    "\n",
    "This notebook shows how to build a GPR model, estimate the parameters $\\theta$ by both maximum likelihood and MCMC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import GPflow\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build a very simple data set:\n",
    "N = 120\n",
    "X = np.random.rand(N,1)\n",
    "Y = np.sin(12*X) + 0.66*np.cos(25*X) + np.random.randn(N,1)*0.1 + 3\n",
    "plt.figure()\n",
    "plt.plot(X, Y, 'kx', mew=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum Likelihood estimation\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#build the GPR object\n",
    "k = GPflow.kernels.Matern52(1)\n",
    "meanf = GPflow.mean_functions.Linear(1,0)\n",
    "m = GPflow.gpr.GPR(X, Y, k, meanf)\n",
    "m.likelihood.variance = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Here are the parameters before optimization\"\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m.optimize()\n",
    "print \"Here are the parameters after optimization\"\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot!\n",
    "xx = np.linspace(-0.1, 1.1, 100)[:,None]\n",
    "mean, var = m.predict_y(xx)\n",
    "plt.figure()\n",
    "plt.plot(X, Y, 'kx', mew=2)\n",
    "plt.plot(xx, mean, 'b', lw=2)\n",
    "plt.plot(xx, mean + 2*np.sqrt(var), 'b--', xx, mean - 2*np.sqrt(var), 'b--', lw=1.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MCMC\n",
    "--\n",
    "First, we'll set come priors on the kernel parameters, then we'll run mcmc and see how much posterior uncertainty there is in the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#we'll choose rather arbitrary priors. \n",
    "m.kern.lengthscales.prior = GPflow.priors.Gamma(1., 1.)\n",
    "m.kern.variance.prior = GPflow.priors.Gamma(1., 1.)\n",
    "m.likelihood.variance.prior = GPflow.priors.Gamma(1., 1.)\n",
    "m.mean_function.A.prior = GPflow.priors.Gaussian(0., 10.)\n",
    "m.mean_function.b.prior = GPflow.priors.Gaussian(0., 10.)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "samples = m.sample(500, epsilon = 0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Note. All these labels are wrong (or, most probably wrong). We need some machinery for labelling posterior samples!\n",
    "\n",
    "f, axs = plt.subplots(1,3, figsize=(12,4), tight_layout=True)\n",
    "axs[0].plot(samples[:,0], samples[:,1], 'k.', alpha = 0.15)\n",
    "axs[0].set_xlabel('noise_variance')\n",
    "axs[0].set_ylabel('signal_variance')\n",
    "axs[1].plot(samples[:,0], samples[:,2], 'k.', alpha = 0.15)\n",
    "axs[1].set_xlabel('noise_variance')\n",
    "axs[1].set_ylabel('lengthscale')\n",
    "axs[2].plot(samples[:,2], samples[:,1], 'k.', alpha = 0.1)\n",
    "axs[2].set_xlabel('lengthscale')\n",
    "axs[2].set_ylabel('signal_variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#an attempt to plot the function posterior\n",
    "#Note that we should really sample the function values here, instead of just using the mean. \n",
    "#We are under-representing the uncertainty here. \n",
    "# TODO: get full_covariance of the predictions (predict_f only?)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for s in samples:\n",
    "    m.set_state(s)\n",
    "    mean, _ = m.predict_y(xx)\n",
    "    plt.plot(xx, mean, 'b', lw=2, alpha = 0.05)\n",
    "    \n",
    "plt.plot(X, Y, 'kx', mew=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
