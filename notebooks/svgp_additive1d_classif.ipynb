{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np   \n",
    "from scipy.special import erf\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import cm \n",
    "%matplotlib inline\n",
    "import GPflow.kernels\n",
    "from GPflow.likelihoods import Bernoulli, Gaussian\n",
    "from GPflow.svgp import SVGP\n",
    "from GPflow.svgp_additive import SVGP_additive2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are interested in fitting generalized additive model.\n",
    "The generative model is the following\n",
    "\n",
    "$$\\begin{align}\n",
    "f_d &\\sim GP(0,k_d(\\theta_d)),\\quad \\forall d \\in [1,D]\\\\\n",
    "\\eta^{(i)} &= \\sum_d f_d(x^{(i)}_d)\\\\\n",
    "y^{(i)} &\\sim p(y|\\eta^{(i)}) \\quad \\text{some known likelihood\n",
    "}.\n",
    "\\end{align}$$\n",
    "\n",
    "Here, $x_d \\in \\cal{X}_d$ and $\\cap_d \\cal{X}_d= \\phi$ , which just means that subsets of covariates for different functions are non overlapping\n",
    "\n",
    "We follow Hensman 2015 (Scalable Variational Gaussian Process Classification) and extend the Posterior approximation: \n",
    "\n",
    "* each Gaussian Process is augmented with inducing points $u_d$ associated to pseudo-inputs $Z_d$\n",
    "\n",
    "* A factorized approximation to the posterior over functions is chosen\n",
    "\n",
    "$$\\begin{align}\n",
    "p(f_1,...,f_D,u_1,...,u_D|Y,X,Z) &\\approx \\prod_d q(f_d,u_d)\\\\\n",
    "&\\approx \\prod_d p(f_d|u_d)q(u_d).\n",
    "\\end{align}$$\n",
    "\n",
    "* Factors over inducing points are chosen to be Gaussian: $q(u_d)=\\cal{N}(\\mu_d,\\Sigma_d)$\n",
    "\n",
    "* We construct a lower bound on the model evidence\n",
    "\n",
    "$$\\log \\,p(Y) \\geq \\sum_i \\mathbb{E}_{\\rho_i} \\log p(y_i|\\rho_i) +\\sum_d KL(q(u_d)||p(u_d))$$\n",
    "\n",
    "where $\\rho_i$ is a univariated Gaussian whose sufficient statistics are functions of $Z,\\mu,\\Sigma, \\theta$\n",
    "\n",
    "* Approximate inference is performed by maximizing the lower bound with respect to\n",
    " * the inducing point locations $Z_d$\n",
    " * the associated variational parameters $\\mu_d,\\Sigma_d$\n",
    " * the kernel hyperparameters $\\theta_d$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "N = 1000 # number of data points\n",
    "D = 3 # number of covariate dimension\n",
    "X = np.random.rand(N, D)-.5 # sampling covariates uniformly\n",
    "\n",
    "# some arbitrary functions    \n",
    "f1 = lambda x : x\n",
    "f2 = lambda x : np.sin(x*5.)\n",
    "f3 = lambda x : -10.*x*np.exp(-np.abs(x)*5.)\n",
    "\n",
    "fns = [f1] # forcing the first covariate to be linear \n",
    "for d in range(1,D):\n",
    "    fns += [random.choice([f2,f3])]\n",
    "    \n",
    "# Computing the additive predictor\n",
    "Fs = np.zeros((N,D))\n",
    "for d in range(D):\n",
    "    Fs[:,d] = fns[d](X[:,d])\n",
    "F = np.sum(Fs,axis=1).reshape(N,1)\n",
    "\n",
    "# Computing the observables\n",
    "\n",
    "lik = 'Gaussian'\n",
    "#lik = 'Bernoulli'\n",
    "\n",
    "if lik == 'Gaussian':\n",
    "    s_n = 1\n",
    "    Y = F + np.random.randn(N,1) * s_n\n",
    "elif lik == 'Bernoulli':\n",
    "    phi =lambda x: 0.5*(1+erf(x/np.sqrt(2)))\n",
    "    B = phi(F)\n",
    "    Y = (np.random.rand(N,1)<B).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting individual functions\n",
    "xp = np.linspace(-.5,.5,200)\n",
    "for d in range(D):\n",
    "    plt.plot(xp,fns[d](xp),'-')\n",
    "plt.title('Individual functions')\n",
    "plt.xlabel('$x_d$',fontsize=20)\n",
    "plt.ylabel('$f_d(x_d)$',fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# histogram of predictor values\n",
    "plt.title('Predictor values')\n",
    "plt.hist(F)\n",
    "plt.xlabel('$\\sum_d f_d$',fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "# Classification only: histogram of Bernoulli parameters\n",
    "if lik == 'Bernoulli':\n",
    "    plt.title('Bernoulli parameters values')\n",
    "    plt.hist(phi(F))\n",
    "    plt.xlabel('$\\phi ( \\sum_d f_d )$',fontsize=20)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inducing point locations \n",
    "Nz = 20\n",
    "Z = [np.array([[1]])] # one pseudo input for linear term\n",
    "for d in range(D-1):\n",
    "    Z+= [np.random.rand(Nz, 1)-.5] # list of (M,1) array\n",
    "    \n",
    "# Setting likelihood\n",
    "if lik == 'Gaussian':\n",
    "    likelihood = Gaussian()\n",
    "    likelihood.variance = 0.01\n",
    "elif lik == 'Bernoulli':\n",
    "    likelihood = Bernoulli()\n",
    "\n",
    "# Setting kernels\n",
    "ks = [GPflow.kernels.Linear(1)]\n",
    "for k in range(1,D):\n",
    "    ks += [ GPflow.kernels.RBF(1)]\n",
    "\n",
    "\n",
    "# Declaring model\n",
    "m = SVGP_additive2(X, Y, ks, likelihood, Z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing parameters\n",
    "Here, we decide which parameters we want to optimize. These include\n",
    "* kernel hyperparameters $\\theta$\n",
    "* inducing point locations $Z$\n",
    "* variational parameters\n",
    "* likelihood parameters (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --- Kernel parameters\n",
    "for k in m.kerns.parameterized_list:\n",
    "    #if k.name == 'linear':\n",
    "    #    k.variance.fixed = True\n",
    "    #if k.name == 'rbf':\n",
    "    #    k.variance.fixed = True\n",
    "    #    k.lengthscales.fixed = True\n",
    "    pass\n",
    "        \n",
    "# --- Inducing points\n",
    "m.Z[0].fixed = True # no need to optimize location for linear parameter\n",
    "#for z in m.Z:\n",
    "#    z.fixed=True\n",
    "\n",
    "# --- Likelihood parameters\n",
    "if lik == 'Gaussian':\n",
    "    #m.likelihood.variance.fixed = True\n",
    "    pass\n",
    "\n",
    "\n",
    "# --- Variational parameters\n",
    "#for qmu in m.q_mu:\n",
    "    #qmu.fixed = True\n",
    "#for qs in m.q_sqrt:\n",
    "    #qs.fixed = True\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# optimizing\n",
    "for k in range(2):\n",
    "    m.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# computing predicted sum (mean and variance)\n",
    "Ds = range(D)\n",
    "m.set_prediction_subset_ds(Ds)\n",
    "\n",
    "Fp, VFp = m.predict_f(X)\n",
    "Yp, VYp = m.predict_y(X)\n",
    "SFp = np.sqrt(VFp)\n",
    "SYp = np.sqrt(VYp)\n",
    "\n",
    "# computing RootMeanSquaredError \n",
    "rmse = np.sqrt(np.mean((Yp - Y) ** 2))\n",
    "print \"RMSE: %.3f\" % rmse\n",
    "\n",
    "# plotting true against inferred predictor\n",
    "n = 100 #subselect plots\n",
    "I = np.random.randint(1,len(Y),n)\n",
    "fig,ax = plt.subplots()\n",
    "ax.errorbar(F,Fp , yerr=np.sqrt(SFp), fmt='o')\n",
    "lims = [ np.min([ax.get_xlim(), ax.get_ylim()]),\n",
    "         np.max([ax.get_xlim(), ax.get_ylim()])]\n",
    "ax.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
    "ax.set_xlabel('underlying predictor',fontsize=20)\n",
    "ax.set_ylabel('estimated predictor mean',fontsize=20)\n",
    "plt.show()\n",
    "# Bernoulli case\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "if lik == 'Bernoulli':\n",
    "    # bin responses per bernoulli param\n",
    "    ax.errorbar(B,Yp , yerr=np.sqrt(SYp), fmt='o')\n",
    "    ax.set_xlabel('true $\\phi(\\sum f)$',fontsize=20)\n",
    "    ax.set_ylabel('predicted $\\phi(\\sum f)$',fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X.shape\n",
    "print m.prediction_ds\n",
    "import tensorflow as tf\n",
    "print tf.expand_dims(X[:, m.f_indices[2]],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generating predictions for individual functions\n",
    "Ys=[]\n",
    "Vs=[]\n",
    "for d in range(D):\n",
    "    m.set_prediction_subset_ds([d])\n",
    "    Yd, Vd = m.predict_f(X)\n",
    "    Ys.append(Yd)\n",
    "    Vs.append(Vd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col=cm.rainbow(np.linspace(0,1,D))\n",
    "fig1,ax1 = plt.subplots()\n",
    "w = 5\n",
    "fig2,axarr = plt.subplots(1,D,figsize=(D*w,D))\n",
    "\n",
    "\n",
    "# plotting infered functions against true functions\n",
    "for d in range(D):\n",
    "    Yd = Ys[d]\n",
    "    Vd = Vs[d]\n",
    "    o = np.argsort(X[:,d])\n",
    "    ax1.plot(X[o,d],Fs[o,d],'--',linewidth=4,c=col[d])\n",
    "    ax1.plot(X[o,d],Yd[o],'-',c=col[d])\n",
    "    ax1.fill_between(X[o,d],\n",
    "                     y1=np.squeeze(Yd[o]+np.sqrt(Vd[o])),\n",
    "                     y2=np.squeeze(Yd[o]-np.sqrt(Vd[o])),facecolor=col[d],alpha=.5)\n",
    "\n",
    "    \n",
    "# plotting infered functions against true functions\n",
    "for d in range(D):\n",
    "    Yd = Ys[d]\n",
    "    Vd = Vs[d]\n",
    "    ax=axarr[d]\n",
    "    ax.errorbar(Fs[o,d], Yd[o], yerr=np.sqrt(Vd), fmt='o')\n",
    "    lims = [ np.min([ax.get_xlim(), ax.get_ylim()]),\n",
    "             np.max([ax.get_xlim(), ax.get_ylim()])]\n",
    "    ax.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
    "    ax.set_xlabel('underlying predictor',fontsize=20)\n",
    "    ax.set_ylabel('estimated predictor mean',fontsize=20)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.zeros((10,10))\n",
    "print x[:,np.array([1,2])].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
